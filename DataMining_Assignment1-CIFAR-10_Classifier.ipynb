{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3117af8",
   "metadata": {},
   "source": [
    "First we import the data and PyTorch libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28533118",
   "metadata": {},
   "source": [
    "After downloading and uploading the CIFAR-10 data images, we then divide the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae756749",
   "metadata": {},
   "source": [
    "We get the following output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e013956",
   "metadata": {},
   "source": [
    "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
    "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
    "Files already downloaded and verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c72f8",
   "metadata": {},
   "source": [
    "Files already downloaded and verified\n",
    "Files already downloaded and verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c1a2",
   "metadata": {},
   "source": [
    "We can now test to see some of the images we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81258c0",
   "metadata": {},
   "source": [
    "We get an output of some images and their classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfb427",
   "metadata": {},
   "source": [
    "Now after testing the downloaded images, we can get to making our Convolutional Neural Network. We are going to use Classification Cross-Entropy loss for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 50, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(50, 100, 5)\n",
    "        self.fc1 = nn.Linear(100 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96b994",
   "metadata": {},
   "source": [
    "We are going to use Classification Cross-Entropy loss for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaef138",
   "metadata": {},
   "source": [
    "We can now start training our Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52fa12",
   "metadata": {},
   "source": [
    "After training our network, we have to save our model. We are going to use this model for further processing. We are going to name our model 'cifar_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d645c0c",
   "metadata": {},
   "source": [
    "Now that we have made the neural network, we are going to test it's performance. To do this, we would make a prediction of the class the neural network is going to output and check it against the ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db649e",
   "metadata": {},
   "source": [
    "We get a certain output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f758a00",
   "metadata": {},
   "source": [
    "Now that we have established the ground truth, we can check what our neural network thinks these images are. Then we can also calculate the accuracy of the model on the whole dataset. To do that, first we can reload our model if it is not already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part is not necessary ###\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "##################################\n",
    "\n",
    "# Letting the neural network classify the testing data set\n",
    "outputs = net(images)\n",
    "\n",
    "# Getting the index of the highest energy\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "\n",
    "# Testing to see how the network performs on the whole dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
